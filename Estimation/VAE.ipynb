{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caccf332",
   "metadata": {},
   "source": [
    "# Masked Evidence Lower Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892227e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebeaed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str):\n",
    "    buffer = None\n",
    "    filepath = os.path.expanduser(filepath)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        buffer = io.StringIO(f.read().replace(\"TRUE\", \"1\").replace(\"FALSE\", \"0\"))\n",
    "    return pd.read_csv(buffer).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "910a67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar60_mask = load_data(\"../Data Simulation/missingness mechanism level data/MCAR60_mask.csv\")\n",
    "mar60_full = load_data(\"../Data Simulation/missingness mechanism level data/MCAR60_X_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7764e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_mask = np.array(np.random.binomial(1, 1/3, (3000,)), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bda7031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar60_full_test = mar60_full[test_train_mask, :]\n",
    "mar60_full_train = mar60_full[test_train_mask ^ True, :]\n",
    "mar60_mask_test = mar60_mask[test_train_mask, :]\n",
    "mar60_mask_train = mar60_mask[test_train_mask ^ True, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcc766ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "cuda = False\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_dim = 20\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aac68d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "test_loader = DataLoader((mar60_full_test, mar60_mask_test), batch_size=batch_size, shuffle=False, **kwargs)\n",
    "train_loader = DataLoader((mar60_full_train, mar60_mask_train), batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5d3e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Gaussian MLP Encoder.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = self.LeakyReLU(self.FC_input(x))\n",
    "        h_ = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean = self.FC_mean(h_)\n",
    "        log_var = self.FC_var(h_)\n",
    "\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d1e8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Gaussian MLP Decoder.\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.FC_hidden(x))\n",
    "        h = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        x_hat = torch.sigmoid(self.FC_output(h))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34e04a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparametrization(self, mean, var):\n",
    "        # Sample from standard normal\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)\n",
    "        # Reparametrization trick\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        # Takes exponential function (log_var -> var)\n",
    "        z = self.reparametrization(mean, torch.exp(.5 * log_var))\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9926e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n",
    "\n",
    "model = Model(encoder=encoder, decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65de82",
   "metadata": {},
   "source": [
    "### 3. Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c29a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var, mask):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(\n",
    "    x_hat,\n",
    "    x.float(),\n",
    "    reduction='none'\n",
    ")\n",
    "    masked_repr_loss = (x_hat * mask).sum()\n",
    "    KLD = -.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return masked_repr_loss + KLD\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29ab85",
   "metadata": {},
   "source": [
    "### 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4f7eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polar0n/Documents/Classes/Research Seminar/animated-palm-tree/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 complete.\tAverage Loss: 13623.083984375\n",
      "\tEpoch 2 complete.\tAverage Loss: 11697.4453125\n",
      "\tEpoch 3 complete.\tAverage Loss: 10910.0087890625\n",
      "\tEpoch 4 complete.\tAverage Loss: 9661.8916015625\n",
      "\tEpoch 5 complete.\tAverage Loss: 8554.458984375\n",
      "\tEpoch 6 complete.\tAverage Loss: 7319.7158203125\n",
      "\tEpoch 7 complete.\tAverage Loss: 6206.599609375\n",
      "\tEpoch 8 complete.\tAverage Loss: 4855.724609375\n",
      "\tEpoch 9 complete.\tAverage Loss: 3599.337158203125\n",
      "\tEpoch 10 complete.\tAverage Loss: 2570.450439453125\n",
      "\tEpoch 11 complete.\tAverage Loss: 1645.6805419921875\n",
      "\tEpoch 12 complete.\tAverage Loss: 1117.165283203125\n",
      "\tEpoch 13 complete.\tAverage Loss: 679.6951904296875\n",
      "\tEpoch 14 complete.\tAverage Loss: 462.72503662109375\n",
      "\tEpoch 15 complete.\tAverage Loss: 275.4583740234375\n",
      "\tEpoch 16 complete.\tAverage Loss: 191.12762451171875\n",
      "\tEpoch 17 complete.\tAverage Loss: 124.2538833618164\n",
      "\tEpoch 18 complete.\tAverage Loss: 81.91263580322266\n",
      "\tEpoch 19 complete.\tAverage Loss: 66.66527557373047\n",
      "\tEpoch 20 complete.\tAverage Loss: 50.389278411865234\n",
      "\tEpoch 21 complete.\tAverage Loss: 44.34241485595703\n",
      "\tEpoch 22 complete.\tAverage Loss: 37.4471549987793\n",
      "\tEpoch 23 complete.\tAverage Loss: 31.626428604125977\n",
      "\tEpoch 24 complete.\tAverage Loss: 26.567157745361328\n",
      "\tEpoch 25 complete.\tAverage Loss: 22.890819549560547\n",
      "\tEpoch 26 complete.\tAverage Loss: 20.27688980102539\n",
      "\tEpoch 27 complete.\tAverage Loss: 18.150299072265625\n",
      "\tEpoch 28 complete.\tAverage Loss: 16.93538475036621\n",
      "\tEpoch 29 complete.\tAverage Loss: 16.007814407348633\n",
      "\tEpoch 30 complete.\tAverage Loss: 15.093626976013184\n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, masks), in enumerate(train_loader):\n",
    "        original_x = x.clone()  # Save original x for loss computation\n",
    "        \n",
    "        x = x.to(DEVICE).float()\n",
    "        masks = masks.to(DEVICE).float()\n",
    "\n",
    "        x = x * masks\n",
    "        x = x.to(DEVICE).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        loss = loss_function(original_x, x_hat, mean, log_var, masks)\n",
    "\n",
    "        overall_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"\\tEpoch {epoch + 1} complete.\\tAverage Loss: {overall_loss / (batch_idx + 1)}\")\n",
    "print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4a74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cc6cf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.0880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_test_loss = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, masks in tqdm.tqdm(test_loader):\n",
    "        x = x.to(DEVICE).float()\n",
    "        masks = masks.to(DEVICE).float()\n",
    "\n",
    "        x = x * masks\n",
    "        x = x.to(DEVICE).float()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        \n",
    "        # Calculate loss for the batch\n",
    "        batch_loss = loss_function(x, x_hat, mean, log_var, masks)\n",
    "\n",
    "        total_test_loss += batch_loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "# Calculate final average error metric\n",
    "average_loss = total_test_loss / total_samples\n",
    "print(f\"Test Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c3d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
