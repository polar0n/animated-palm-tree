{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caccf332",
   "metadata": {},
   "source": [
    "# Masked Evidence Lower Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "892227e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ebeaed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str):\n",
    "    buffer = None\n",
    "    filepath = os.path.expanduser(filepath)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        buffer = io.StringIO(f.read().replace(\"TRUE\", \"1\").replace(\"FALSE\", \"0\"))\n",
    "    return pd.read_csv(buffer).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "910a67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar60_mask = load_data(\"../Data Simulation/missingness mechanism level data/MCAR60_mask.csv\")\n",
    "mar60_full = load_data(\"../Data Simulation/missingness mechanism level data/MCAR60_X_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7764e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "test_train_mask = np.array(np.random.binomial(1, 1/3, (3000,)), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bda7031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar60_full_test = mar60_full[test_train_mask, :]\n",
    "mar60_full_train = mar60_full[test_train_mask ^ True, :]\n",
    "mar60_mask_test = mar60_mask[test_train_mask, :]\n",
    "mar60_mask_train = mar60_mask[test_train_mask ^ True, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dcc766ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "cuda = False\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_dim = 20\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aac68d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "test_loader = DataLoader((mar60_full_test, mar60_mask_test), batch_size=batch_size, shuffle=False, **kwargs)\n",
    "train_loader = DataLoader((mar60_full_train, mar60_mask_train), batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5d3e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Gaussian MLP Encoder.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = self.LeakyReLU(self.FC_input(x))\n",
    "        h_ = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean = self.FC_mean(h_)\n",
    "        log_var = self.FC_var(h_)\n",
    "\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d1e8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Gaussian MLP Decoder.\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.FC_hidden(x))\n",
    "        h = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        x_hat = torch.sigmoid(self.FC_output(h))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34e04a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparametrization(self, mean, var):\n",
    "        # Sample from standard normal\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)\n",
    "        # Reparametrization trick\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        # Takes exponential function (log_var -> var)\n",
    "        z = self.reparametrization(mean, torch.exp(.5 * log_var))\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9926e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n",
    "\n",
    "model = Model(encoder=encoder, decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65de82",
   "metadata": {},
   "source": [
    "### 3. Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c29a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var, mask):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(\n",
    "    x_hat,\n",
    "    x.float(),\n",
    "    reduction='none'\n",
    ")\n",
    "    masked_repr_loss = (x_hat * mask).sum()\n",
    "    KLD = -.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return masked_repr_loss + KLD\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29ab85",
   "metadata": {},
   "source": [
    "### 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d4f7eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polar0n/Documents/Classes/Research Seminar/animated-palm-tree/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 complete.\tAverage Loss: 14039.408203125\n",
      "\tEpoch 2 complete.\tAverage Loss: 12025.8388671875\n",
      "\tEpoch 3 complete.\tAverage Loss: 10837.6630859375\n",
      "\tEpoch 4 complete.\tAverage Loss: 9767.6904296875\n",
      "\tEpoch 5 complete.\tAverage Loss: 8607.30078125\n",
      "\tEpoch 6 complete.\tAverage Loss: 7400.46533203125\n",
      "\tEpoch 7 complete.\tAverage Loss: 6090.17822265625\n",
      "\tEpoch 8 complete.\tAverage Loss: 4996.54638671875\n",
      "\tEpoch 9 complete.\tAverage Loss: 3587.431396484375\n",
      "\tEpoch 10 complete.\tAverage Loss: 2555.0224609375\n",
      "\tEpoch 11 complete.\tAverage Loss: 1869.779052734375\n",
      "\tEpoch 12 complete.\tAverage Loss: 1253.478271484375\n",
      "\tEpoch 13 complete.\tAverage Loss: 790.1278076171875\n",
      "\tEpoch 14 complete.\tAverage Loss: 509.42083740234375\n",
      "\tEpoch 15 complete.\tAverage Loss: 341.7733154296875\n",
      "\tEpoch 16 complete.\tAverage Loss: 226.39454650878906\n",
      "\tEpoch 17 complete.\tAverage Loss: 156.79315185546875\n",
      "\tEpoch 18 complete.\tAverage Loss: 120.42412567138672\n",
      "\tEpoch 19 complete.\tAverage Loss: 99.82807922363281\n",
      "\tEpoch 20 complete.\tAverage Loss: 83.28691101074219\n",
      "\tEpoch 21 complete.\tAverage Loss: 72.00761413574219\n",
      "\tEpoch 22 complete.\tAverage Loss: 59.42457962036133\n",
      "\tEpoch 23 complete.\tAverage Loss: 52.093040466308594\n",
      "\tEpoch 24 complete.\tAverage Loss: 44.519622802734375\n",
      "\tEpoch 25 complete.\tAverage Loss: 35.89825439453125\n",
      "\tEpoch 26 complete.\tAverage Loss: 37.08098220825195\n",
      "\tEpoch 27 complete.\tAverage Loss: 32.71343231201172\n",
      "\tEpoch 28 complete.\tAverage Loss: 33.779937744140625\n",
      "\tEpoch 29 complete.\tAverage Loss: 29.85696029663086\n",
      "\tEpoch 30 complete.\tAverage Loss: 27.8858642578125\n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, masks), in enumerate(train_loader):\n",
    "        original_x = x.clone()  # Save original x for loss computation\n",
    "        \n",
    "        x = x.to(DEVICE).float()\n",
    "        # Rescaling the data from {0,1} to {-1,1}\n",
    "        x = x * 2 - 1\n",
    "        masks = masks.to(DEVICE).float()\n",
    "\n",
    "        x = x * masks\n",
    "        x = x.to(DEVICE).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        loss = loss_function(original_x, x_hat, mean, log_var, masks)\n",
    "\n",
    "        overall_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"\\tEpoch {epoch + 1} complete.\\tAverage Loss: {overall_loss / (batch_idx + 1)}\")\n",
    "print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4a74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6cc6cf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 12.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_test_loss = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, masks in tqdm.tqdm(test_loader):\n",
    "        original_x = x.clone()  # Save original x for loss computation\n",
    "        \n",
    "        x = x.to(DEVICE).float()\n",
    "        # Rescaling the data from {0,1} to {-1,1}\n",
    "        x = x * 2 - 1\n",
    "        masks = masks.to(DEVICE).float()\n",
    "\n",
    "        x = x * masks\n",
    "        x = x.to(DEVICE).float()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    " \n",
    "        # Calculate loss for the batch\n",
    "        batch_loss = loss_function(original_x, x_hat, mean, log_var, masks)\n",
    "\n",
    "        total_test_loss += batch_loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "# Calculate final average error metric\n",
    "average_loss = total_test_loss / total_samples\n",
    "print(f\"Test Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc43fb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, ..., 1, 0, 0],\n",
       "       [1, 1, 0, ..., 1, 1, 1],\n",
       "       [0, 1, 1, ..., 1, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 1]], shape=(992, 20))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mar60_full_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "af5e89e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1,  0, ...,  0, -1, -1],\n",
       "       [ 0,  1, -1, ...,  1,  1,  1],\n",
       "       [-1,  0,  0, ..., -1,  0, -1],\n",
       "       ...,\n",
       "       [ 0, -1,  0, ..., -1, -1, -1],\n",
       "       [-1,  1,  0, ...,  1, -1,  0],\n",
       "       [-1,  1,  1, ...,  1,  0,  1]], shape=(1003, 20))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mar60_full_test * 2 - 1) * mar60_mask_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43890ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
