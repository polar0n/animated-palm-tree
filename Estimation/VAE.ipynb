{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caccf332",
   "metadata": {},
   "source": [
    "# Masked Evidence Lower Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892227e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebeaed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str):\n",
    "    buffer = None\n",
    "    filepath = os.path.expanduser(filepath)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        buffer = io.StringIO(f.read().replace(\"TRUE\", \"1\").replace(\"FALSE\", \"0\"))\n",
    "    return pd.read_csv(buffer).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "910a67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar60_mask = load_data(\"../Data Simulation/missingness mechanism level data/MNAR60_mask.csv\")\n",
    "mar60_full = load_data(\"../Data Simulation/missingness mechanism level data/MNAR60_X_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7764e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_mask = np.array(np.random.binomial(1, 1/3, (3000,)), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda7031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar60_full_test = mar60_full[test_train_mask, :]\n",
    "mar60_full_train = mar60_full[test_train_mask ^ True, :]\n",
    "mar60_mask_test = mar60_mask[test_train_mask, :]\n",
    "mar60_mask_train = mar60_mask[test_train_mask ^ True, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcc766ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "cuda = False\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_dim = 20\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac68d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "test_loader = DataLoader((mar60_full_test, mar60_mask_test), batch_size=batch_size, shuffle=False, **kwargs)\n",
    "train_loader = DataLoader((mar60_full_train, mar60_mask_train), batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5d3e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Gaussian MLP Encoder.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = self.LeakyReLU(self.FC_input(x))\n",
    "        h_ = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean = self.FC_mean(h_)\n",
    "        log_var = self.FC_var(h_)\n",
    "\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d1e8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Gaussian MLP Decoder.\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.FC_hidden(x))\n",
    "        h = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        x_hat = torch.sigmoid(self.FC_output(h))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e04a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparametrization(self, mean, var):\n",
    "        # Sample from standard normal\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)\n",
    "        # Reparametrization trick\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        # Takes exponential function (log_var -> var)\n",
    "        z = self.reparametrization(mean, torch.exp(.5 * log_var))\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9926e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n",
    "\n",
    "model = Model(encoder=encoder, decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65de82",
   "metadata": {},
   "source": [
    "### 3. Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c29a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var, mask):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(\n",
    "    x_hat,\n",
    "    x.float(),\n",
    "    reduction='none'\n",
    ")\n",
    "    masked_repr_loss = (x_hat * mask).sum()\n",
    "    KLD = -.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return masked_repr_loss + KLD\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29ab85",
   "metadata": {},
   "source": [
    "### 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4f7eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polar0n/Documents/Classes/Research Seminar/animated-palm-tree/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 complete.\tAverage Loss: 12965.9921875\n",
      "\tEpoch 2 complete.\tAverage Loss: 11435.9853515625\n",
      "\tEpoch 3 complete.\tAverage Loss: 10753.2060546875\n",
      "\tEpoch 4 complete.\tAverage Loss: 9763.6826171875\n",
      "\tEpoch 5 complete.\tAverage Loss: 8618.2060546875\n",
      "\tEpoch 6 complete.\tAverage Loss: 7344.2587890625\n",
      "\tEpoch 7 complete.\tAverage Loss: 5661.4873046875\n",
      "\tEpoch 8 complete.\tAverage Loss: 4291.87548828125\n",
      "\tEpoch 9 complete.\tAverage Loss: 3361.4453125\n",
      "\tEpoch 10 complete.\tAverage Loss: 2312.8134765625\n",
      "\tEpoch 11 complete.\tAverage Loss: 1397.0665283203125\n",
      "\tEpoch 12 complete.\tAverage Loss: 892.0467529296875\n",
      "\tEpoch 13 complete.\tAverage Loss: 575.6755981445312\n",
      "\tEpoch 14 complete.\tAverage Loss: 356.8083801269531\n",
      "\tEpoch 15 complete.\tAverage Loss: 227.0950164794922\n",
      "\tEpoch 16 complete.\tAverage Loss: 159.65879821777344\n",
      "\tEpoch 17 complete.\tAverage Loss: 105.37462615966797\n",
      "\tEpoch 18 complete.\tAverage Loss: 68.76162719726562\n",
      "\tEpoch 19 complete.\tAverage Loss: 54.19976043701172\n",
      "\tEpoch 20 complete.\tAverage Loss: 48.591468811035156\n",
      "\tEpoch 21 complete.\tAverage Loss: 41.986270904541016\n",
      "\tEpoch 22 complete.\tAverage Loss: 36.55057907104492\n",
      "\tEpoch 23 complete.\tAverage Loss: 31.433807373046875\n",
      "\tEpoch 24 complete.\tAverage Loss: 27.370336532592773\n",
      "\tEpoch 25 complete.\tAverage Loss: 23.77106475830078\n",
      "\tEpoch 26 complete.\tAverage Loss: 21.122089385986328\n",
      "\tEpoch 27 complete.\tAverage Loss: 18.907136917114258\n",
      "\tEpoch 28 complete.\tAverage Loss: 17.346765518188477\n",
      "\tEpoch 29 complete.\tAverage Loss: 15.973481178283691\n",
      "\tEpoch 30 complete.\tAverage Loss: 14.881019592285156\n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, masks), in enumerate(train_loader):\n",
    "        original_x = x.clone()  # Save original x for loss computation\n",
    "        \n",
    "        x = x.to(DEVICE).float()\n",
    "        masks = masks.to(DEVICE).float()\n",
    "\n",
    "        x = x * masks\n",
    "        x = x.to(DEVICE).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        loss = loss_function(original_x, x_hat, mean, log_var, masks)\n",
    "\n",
    "        overall_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"\\tEpoch {epoch + 1} complete.\\tAverage Loss: {overall_loss / (batch_idx + 1)}\")\n",
    "print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4a74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6cf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_test_loss = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, masks in tqdm.tqdm(test_loader):\n",
    "        x = x.to(DEVICE).float()\n",
    "        masks = masks.to(DEVICE).float()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        \n",
    "        # Calculate loss for the batch\n",
    "        batch_loss = loss_function(x, x_hat, mean, log_var, masks)\n",
    "\n",
    "        total_test_loss += batch_loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "# Calculate final average error metric\n",
    "average_loss = total_test_loss / total_samples\n",
    "print(f\"Test Loss: {average_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
